# -*- coding: utf-8 -*-
"""Verlon_R_AMMI_Bert.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NAMr1IO4AKVLE0Um25vddiqqsDOau5dS

# Tite : Opinion Mining of Ukraine-Russia War Tweets Using Deep Learning

## Let us perform Sentiment Analysis with BERT

## Setup
"""

!pip install -q -U watermark

!pip install -qq transformers

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext watermark
# %watermark -v -p numpy,pandas,torch,transformers

"""##Import Librairies & Configuration"""

# Commented out IPython magic to ensure Python compatibility.

import transformers
from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup
import torch

import numpy as np
import pandas as pd
import seaborn as sns
from pylab import rcParams
import matplotlib.pyplot as plt
from matplotlib import rc
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from collections import defaultdict
from textwrap import wrap

from torch import nn, optim
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F

# %matplotlib inline
# %config InlineBackend.figure_format='retina'

sns.set(style='whitegrid', palette='muted', font_scale=1.2)

HAPPY_COLORS_PALETTE = ["#01BEFE", "#FFDD00", "#FF7D00", "#FF006D", "#ADFF02", "#8F00FF"]

sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))

rcParams['figure.figsize'] = 8, 6

RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
torch.manual_seed(RANDOM_SEED)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
device

"""##Data Exploration"""

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv("/content/drive/MyDrive/Opinion Mining of Ukraine-Russia War Tweets Using Deep Learning/data/processed/labeled_sample_600.csv")
df.head()

df.shape

df.info()

df.isnull().sum()

df.columns

df["value"].value_counts()

df["value"]

# The 'value' column of DataFrame df contains values 0, 1, and 2
sns.countplot(data=df, x='value', order=[0, 1, 2])
plt.xlabel('review score')
plt.show()

## Define a function to convert ratings to sentiment labels
def to_sentiment(rating):
  rating = int(rating)      # Convert the rating to an integer (in case it's not already)
  if rating == 0:          # Check if the rating is 0
    return 0               # If it is, return sentiment label 0
  elif rating == 1:        # Check if the rating is 1
    return 1               # If it is, return sentiment label 1
  else:                    # If the rating is neither 0 nor 1
    return 2               # Return sentiment label 2

# Apply the to_sentiment function to the 'value' column of the DataFrame 'df'
# and create a new column named 'sentiment' to store the sentiment labels.

df['sentiment'] = df.value.apply(to_sentiment)

df['sentiment']

# Assuming the 'sentiment' column of DataFrame df contains sentiment labels (0, 1, 2)
ax = sns.countplot(data=df, x='sentiment')

# Set the x-axis label
plt.xlabel('review sentiment')

# Class_names is a list containing the corresponding class labels
class_names = ['negative', 'neutral', 'positive']

# Set the x-axis tick labels to the class names
ax.set_xticklabels(class_names)

# Show the plot
plt.show()

# Create a DataFrame containing the counts of each unique value in the 'sentiment' column using the groupby function.
grouped_counts = df.groupby(['sentiment'])['sentiment'].count()

# Convert the grouped_counts Series back to a DataFrame and rename the 'sentiment' column to 'Counts'.
# The DataFrame will have two columns: 'sentiment' (now renamed as 'Counts') and the count of each sentiment.
counts_df = pd.DataFrame(grouped_counts).rename(columns={"sentiment": "Counts"})

# Calculate the percentage of each sentiment class relative to the total number of sentiments in the DataFrame.
# The calculation is performed using the 'assign' method and a lambda function 'Percentage'.
# The 'Percentage' column is added to the counts_df DataFrame.
counts_df = counts_df.assign(Percentage=lambda x: (x.Counts / x.Counts.sum()) * 100)
counts_df

# Define the counts for each sentiment category (positive, neutral, and negative).
positive = 280
neutral = 155
negative = 165

# Creating a Pie Chart to visualize the distribution of sentiments.
# Define the labels for the pie chart, including the counts for each sentiment category.
labels = ['Positive [' + str(positive) + ']', 'Neutral [' + str(neutral) + ']', 'Negative [' + str(negative) + ']']

# Define the sizes of the pie chart slices, representing the counts for each sentiment category.
sizes = [positive, neutral, negative]

# Define colors for each sentiment category in the pie chart.
colors = ["#81F495", "#A9E4EF", "#FF3C38"]

# Create the pie chart using the sizes and colors defined above.
patches, texts = plt.pie(sizes, colors=colors, startangle=90)

# Use the default style for the plot.
plt.style.use('default')

# Add a legend to the pie chart with the specified labels.
plt.legend(labels)

# Set the title for the pie chart.
plt.title('#Number of Tweets (Positive, Negative, Neutral)')

# Ensure the pie chart is circular (aspect ratio equal) and show the plot.
plt.axis('equal')
plt.show()



# Calculate the length of each text in the 'text' column and add it as a new column 'text_len' in the DataFrame (df).
# The 'astype(str)' ensures that each text is converted to a string type before applying the 'len' function.
# The result is the number of characters in each text, which is stored in the new 'text_len' column.
df['text_len'] = df['text'].astype(str).apply(len)

# Calculate the word count of each text in the 'text' column and add it as a new column 'text_word_count' in the DataFrame (df).
# The 'lambda x: len(str(x).split())' function splits each text into words using whitespace as the delimiter,
# and then calculates the number of words in each text.
# The result is the word count for each text, which is stored in the new 'text_word_count' column.
df['text_word_count'] = df['text'].apply(lambda x: len(str(x).split()))

# Print the average length of text across all rows in the DataFrame (df).
# The 'np.mean(df['text_len'])' calculates the mean of the 'text_len' column.
# The 'round' function is used to round the average length to the nearest integer.
print("Average length of text:", round(np.mean(df['text_len'])))

# Print the average word count of text across all rows in the DataFrame (df).
# The 'np.mean(df['text_word_count'])' calculates the mean of the 'text_word_count' column.
# The 'round' function is used to round the average word count to the nearest integer.
print("Average word counts of text:", round(np.mean(df['text_word_count'])))

print(df.columns)

"""##Data Preparation"""

df.nlargest(n=50, columns=['sentiment'])["text"]

PRE_TRAINED_MODEL_NAME = 'bert-base-cased'

tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)

sample_txt = 'When was I last outside? I am stuck at home for 2 weeks.'

tokens = tokenizer.tokenize(sample_txt)
token_ids = tokenizer.convert_tokens_to_ids(tokens)

print(f' Sentence: {sample_txt}')
print(f'   Tokens: {tokens}')
print(f'Token IDs: {token_ids}')

"""### Special Tokens"""

#The tokenizer.sep_token represents the separator token used to separate different segments of text
#,and tokenizer.sep_token_id gives its corresponding integer ID in the tokenizer's vocabulary.

tokenizer.sep_token, tokenizer.sep_token_id

#The tokenizer.cls_token is the special classification token used as a prefix in text sequences, and
#tokenizer.cls_token_id is its corresponding integer ID in the tokenizer's vocabulary.

tokenizer.cls_token, tokenizer.cls_token_id

#  tokenizer.pad_token: This represents the special token used by the tokenizer to indicate padding.

#  tokenizer.pad_token_id: This is the numeric ID associated with the pad_token by the tokenizer.
tokenizer.pad_token, tokenizer.pad_token_id

#Tokenizer.unk_token: This represents the special token used by the tokenizer to indicate unknown or out-of-vocabulary (OOV) tokens.

#Tokenizer.unk_token_id: This is the numeric ID associated with the unk_token by the tokenizer, used when an unknown token is encountered during tokenization.
tokenizer.unk_token, tokenizer.unk_token_id

#  tokenizer.encode_plus(...): This line uses the tokenizer to encode the 'sample_txt'
 #text into numerical tokens and other useful features like attention mask.
encoding = tokenizer.encode_plus(
    sample_txt,                  # The text to be tokenized and encoded.
    max_length=32,               # The maximum length of the encoded sequence.
    add_special_tokens=True,     # Add special tokens like '[CLS]' and '[SEP]' to the encoded sequence.
    return_token_type_ids=False, # Do not return token type IDs, useful for single-sequence tasks.
    pad_to_max_length=True,      # Pad the sequence to the 'max_length' with padding tokens.
    return_attention_mask=True,  # Return an attention mask, indicating which tokens are valid and which are padded.
    return_tensors='pt',         # Return PyTorch tensors for the encoded sequence and attention mask.
)

# Comment for encoding.keys(): This line prints the keys of the 'encoding' dictionary, showing the
#available information after encoding.
print(encoding.keys())

#  This line prints the length of the tokenized and encoded input sequence.
print(len(encoding['input_ids'][0]))

#  This line accesses the 'input_ids' key of the 'encoding' dictionary and retrieves the encoded input sequence as a list.
encoding['input_ids'][0]

# Access the 'attention_mask' key of the 'encoding' dictionary and retrieve the attention mask for
#the encoded input sequence.
attention_mask = encoding['attention_mask']

#  This line prints the length of the attention mask, which indicates the valid tokens in the encoded
#input sequence.
print(len(attention_mask[0]))
encoding['attention_mask']

# Access the 'input_ids' key of the 'encoding' dictionary and retrieve the encoded input sequence as a list of token IDs.
input_ids = encoding['input_ids'][0]

#  tokenizer.convert_ids_to_tokens(input_ids): This line converts the token IDs back to their corresponding tokens.
# It is used to map the numerical representation of tokens back to their original text form.
# This step is helpful for human interpretation and analysis of the tokenized sequence.
# It is also useful to understand how the text has been tokenized, especially when dealing with special tokens, such as '[CLS]' and '[SEP]'.
tokens = tokenizer.convert_ids_to_tokens(input_ids)

# The 'tokens' variable now contains the list of tokens in their original text form,
# making it easier to understand and analyze the tokenization process.
# You can print 'tokens' to see the resulting list of tokens.
print(tokens)

"""##Choice of  Sequence Length"""

# Calculate the lengths of tokenized sequences and store them in the 'token_lens' list.
token_lens = [len(tokenizer.encode(txt, max_length=512)) for txt in df.text]

sns.distplot(token_lens)
plt.xlim([0, 256]);
plt.xlabel('Token count');

MAX_LEN = 160

import torch
from torch.utils.data import Dataset

# Define a custom dataset class named GPReviewDataset that inherits from torch.utils.data.Dataset.
class GPReviewDataset(Dataset):
    def __init__(self, reviews, targets, tokenizer, max_len):
        # Constructor method that initializes the dataset with the given reviews, targets, tokenizer, and max_len.
        self.reviews = reviews
        self.targets = targets
        self.tokenizer = tokenizer
        self.max_len = max_len

    # The __len__ method returns the length of the dataset, i.e., the number of samples in the dataset.
    def __len__(self):
        return len(self.reviews)

    # The __getitem__ method retrieves a specific item from the dataset.
    def __getitem__(self, item):
        # Get the review text and target label for the specified item in the dataset.
        review = str(self.reviews[item])
        target = self.targets[item]

        # Tokenize the review text using the tokenizer with the specified max_len.
        encoding = self.tokenizer.encode_plus(
            review,
            add_special_tokens=True,    # Add special tokens like [CLS] and [SEP].
            max_length=self.max_len,    # Limit the maximum length of the tokenized sequence.
            padding='max_length',       # Pad sequences to the max length if they are shorter.
            return_attention_mask=True, # Return attention mask to distinguish valid tokens from padded tokens.
            return_tensors='pt',        # Return PyTorch tensors for the tokenized sequence and attention mask.
            truncation=True             # Truncate sequences longer than max_len to fit the specified maximum length.
        )

        # Return a dictionary containing the review text, tokenized input IDs, attention mask, and target label.
        return {
            'review_text': review,                          # Original review text.
            'input_ids': encoding['input_ids'].flatten(),   # Flattened tensor of tokenized input IDs.
            'attention_mask': encoding['attention_mask'].flatten(),  # Flattened tensor of attention mask.
            'targets': torch.tensor(target, dtype=torch.long)       # Target label as a PyTorch tensor.
        }

df_train, df_test = train_test_split(df, test_size=0.1, random_state=RANDOM_SEED)
df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=RANDOM_SEED)

df_train.shape, df_val.shape, df_test.shape

from torch.utils.data import DataLoader

def create_data_loader(df, tokenizer, max_len, batch_size):
    # Create a GPReviewDataset instance with the reviews and targets from the DataFrame.
    # The GPReviewDataset is a custom Dataset class that handles tokenization and data preparation.
    ds = GPReviewDataset(
        reviews=df.text.to_numpy(),       # Convert the 'text' column of the DataFrame to a NumPy array for the GPReviewDataset.
        targets=df.sentiment.to_numpy(),   # Convert the 'sentiment' column of the DataFrame to a NumPy array for the GPReviewDataset.
        tokenizer=tokenizer,              # Pass the tokenizer object to the GPReviewDataset for text tokenization.
        max_len=max_len                   # The maximum length of tokenized sequences for the GPReviewDataset.
    )

    # Return a DataLoader with the GPReviewDataset and the specified batch_size and num_workers.
    # DataLoader is used to batch and prepare the data for model training or evaluation.
    # It takes in the GPReviewDataset instance and returns batches of data with the specified batch_size.
    # The 'num_workers' argument determines the number of worker processes for data loading, which can speed up data loading for large datasets.
    return DataLoader(
        ds,
        batch_size=batch_size,
        num_workers=4
    )

BATCH_SIZE = 16

train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)
val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)
test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)

# Retrieve the next batch of data from the 'train_data_loader' using the 'next' function and store it in the 'data' variable.
data = next(iter(train_data_loader))

#  data.keys()  accesses the keys of the 'data' dictionary, showing the available items in the batch.
# The 'data' dictionary typically contains items like 'input_ids', 'attention_mask', and 'targets' used in model training.
# These items represent tokenized input, attention masks, and target labels, respectively.
# You can print 'data.keys()' to see the available keys and understand the structure of the batch.
data.keys()

print(data['input_ids'].shape)
print(data['attention_mask'].shape)
print(data['targets'].shape)

"""## Let us perform now Sentiment Classification with BERT and Hugging Face"""

bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)

# Pass the input_ids and attention_mask from the 'data' dictionary as inputs to the 'bert_model'.
# The 'bert_model' is a pre-trained BERT model that processes input sequences and produces various outputs.
output = bert_model(
    input_ids=data['input_ids'],         # Tokenized input IDs of the batch obtained from the DataLoader.
    attention_mask=data['attention_mask'] # Attention mask indicating valid tokens and padding tokens.
)
output.last_hidden_state
output.pooler_output

#The following code retrieves and prints the shape of the last hidden state tensor (output.last_hidden_state),
#showing the dimensions of batch size, sequence length, and hidden size for further analysis and processing.
output.last_hidden_state.shape

"""The previous result indicates that the last hidden state tensor has a size of 16 samples in the batch, each with a sequence length of 160 tokens, and each token representation has a hidden size of 768 dimensions."""

#hidden size of the BERT model's representations
# the dimensionality of the hidden states and embeddings produced by the BERT model.
bert_model.config.hidden_size

# The following code  prints the shape of the pooled output tenso
#which represents the aggregated representation of the entire input sequence and has a shape of
 #(batch_size, hidden_size), where 'batch_size' is the number of samples in the batch, and 'hidden_size'
 #is the size of the hidden representations of the BERT model.
output.pooler_output.shape

class SentimentClassifier(nn.Module):
    def __init__(self, n_classes):
        super(SentimentClassifier, self).__init__()
        self.bert = BertModel.from_pretrained('bert-base-cased', return_dict=False)
        self.drop = nn.Dropout(p=0.3)
        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)

    def forward(self, input_ids, attention_mask):
        # Pass the input_ids and attention_mask through the BERT model.
        _, pooled_output = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask
        )
        # Apply dropout to the pooled_output.
        output = self.drop(pooled_output)
        # Pass the output through the final classification layer.
        return self.out(output)

model = SentimentClassifier(len(class_names))
model = model.to(device)

input_ids = data['input_ids'].to(device)
attention_mask = data['attention_mask'].to(device)

print(input_ids.shape) # batch size x seq length
print(attention_mask.shape) # batch size x seq length

F.softmax(model(input_ids, attention_mask), dim=1)

"""The given code defines a custom PyTorch module SentimentClassifier for sentiment classification using a pre-trained BERT model. It includes a dropout layer to prevent overfitting and a linear layer for the final classification. The forward method performs the model's forward pass, returning the class probabilities using the softmax function for the given input batch.

### Now we start with the training
"""

import torch.optim as optim

Epochs = 10

# Define the optimizer for the model using the AdamW optimizer with a learning rate of 2e-5 and no bias correction.
optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)

# Calculate the total number of training steps across all epochs.
total_steps = len(train_data_loader) * Epochs

# Create a learning rate scheduler using the linear scheduler with warm-up.
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=0,
    num_training_steps=total_steps
)

# Define the loss function for the classification task, which is the cross-entropy loss.
# The loss function is moved to the device (e.g., GPU) for computation.
loss_fn = nn.CrossEntropyLoss().to(device)

def train_epoch(
    model, data_loader, loss_fn, optimizer, device, scheduler, n_examples
):
    # Set the model in training mode.
    model = model.train()

    # Initialize lists to store losses and count correct predictions.
    losses = []
    correct_predictions = 0

    # Iterate over batches in the data loader.
    for i in data_loader:
        # Move input tensors to the device (GPU, if available).
        input_ids = i["input_ids"].to(device)
        attention_mask = i["attention_mask"].to(device)
        targets = i["targets"].to(device)

        # Forward pass: compute model outputs.
        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask
        )

        # Compute the predictions (class indices with highest probabilities).
        _, preds = torch.max(outputs, dim=1)

        # Calculate the loss between model predictions and target labels.
        loss = loss_fn(outputs, targets)

        # Count the number of correct predictions.
        correct_predictions += torch.sum(preds == targets)

        # Store the loss value for this batch.
        losses.append(loss.item())

        # Backpropagation: compute gradients and update model parameters.
        loss.backward()
        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()

        # Learning rate scheduler update.
        scheduler.step()

        # Reset the gradients to zero for the next iteration.
        optimizer.zero_grad()

    # Calculate the accuracy and average loss for the entire epoch.
    accuracy = correct_predictions.double() / n_examples
    average_loss = np.mean(losses)

    return accuracy, average_loss

def eval_model(model, data_loader, loss_fn, device, n_examples):
    # Set the model in evaluation mode (no gradient computation and no parameter updates).
    model = model.eval()

    # Initialize lists to store losses and count correct predictions.
    losses = []
    correct_predictions = 0

    # Use 'torch.no_grad()' to disable gradient computation for faster evaluation.
    with torch.no_grad():
        # Iterate over batches in the data loader.
        for i in data_loader:
            # Move input tensors to the device (GPU, if available).
            input_ids = i["input_ids"].to(device)
            attention_mask = i["attention_mask"].to(device)
            targets = i["targets"].to(device)

            # Forward pass: compute model outputs.
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask
            )

            # Compute the predictions (class indices with highest probabilities).
            _, preds = torch.max(outputs, dim=1)

            # Calculate the loss between model predictions and target labels.
            loss = loss_fn(outputs, targets)

            # Count the number of correct predictions.
            correct_predictions += torch.sum(preds == targets)

            # Store the loss value for this batch.
            losses.append(loss.item())

    # Calculate the accuracy and average loss for the entire evaluation dataset.
    accuracy = correct_predictions.double() / n_examples
    average_loss = np.mean(losses)

    return accuracy, average_loss

# Initialize lists to store training history.
history = {
    'train_acc': [],
    'train_loss': [],
    'val_acc': [],
    'val_loss': []
}

# Initialize the best accuracy for early stopping and model saving.
best_accuracy = 0.0

for epoch in range(Epochs):
    # Print the current epoch number.
    print(f'Epoch {epoch + 1}/{Epochs}')
    print('-' * 10)

    # Train the model and obtain training accuracy and loss for the epoch.
    train_acc, train_loss = train_epoch(
        model,
        train_data_loader,
        loss_fn,
        optimizer,
        device,
        scheduler,
        len(df_train)
    )

    # Print training loss and accuracy for the epoch.
    print(f'Train loss {train_loss:.4f} accuracy {train_acc:.4f}')

    # Evaluate the model on the validation dataset and obtain accuracy and loss.
    val_acc, val_loss = eval_model(
        model,
        val_data_loader,
        loss_fn,
        device,
        len(df_val)
    )

    # Print validation loss and accuracy for the epoch.
    print(f'Val   loss {val_loss:.4f} accuracy {val_acc:.4f}')
    print()

    # Append the training history for plotting and analysis.
    history['train_acc'].append(train_acc)
    history['train_loss'].append(train_loss)
    history['val_acc'].append(val_acc)
    history['val_loss'].append(val_loss)

    # Check if the current validation accuracy is better than the previous best accuracy.
    # If yes, save the model's state to a file for early stopping and model saving.
    if val_acc > best_accuracy:
        torch.save(model.state_dict(), 'best_model_state.bin')
        best_accuracy = val_acc

# Evaluate the trained model on the test dataset and obtain accuracy.
test_acc, _ = eval_model(
    model,
    test_data_loader,
    loss_fn,
    device,
    len(df_test)
)

# Convert the test accuracy to a Python float value and print it.
test_accuracy_value = test_acc.item()
print(f"Test accuracy: {test_accuracy_value:.4f}")

"""The accuracy is about 3% lower on the test set. Our model seems to generalize well.

We'll define a helper function to get the predictions from our model:
"""

# Move the tensors to CPU to avoid converting CUDA tensors to NumPy arrays
train_acc = torch.tensor(history['train_acc']).cpu()
val_acc = torch.tensor(history['val_acc']).cpu()

# Plot the training and validation accuracy
plt.plot(train_acc, label='train accuracy')
plt.plot(val_acc, label='validation accuracy')

plt.title('Training history')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend()
plt.ylim([0, 1])

plt.show()

"""The training accuracy starts to approach 100% after 8 epochs or so. You might try to fine-tune the parameters a bit more."""



import torch
import torch.nn.functional as F

def get_predictions(model, data_loader, device):
    # Set the model in evaluation mode (no gradient computation and no parameter updates).
    model = model.eval()

    # Initialize lists to store review texts, predictions, prediction probabilities, and real values.
    review_texts = []
    predictions = []
    prediction_probs = []
    real_values = []

    # Use 'torch.no_grad()' to disable gradient computation for faster inference.
    with torch.no_grad():
        # Iterate over batches in the data loader.
        for i in data_loader:
            # Get the review texts, input tensors, and target labels from the data loader.
            texts = i["review_text"]
            input_ids = i["input_ids"].to(device)
            attention_mask = i["attention_mask"].to(device)
            targets = i["targets"].to(device)

            # Forward pass: compute model outputs.
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask
            )

            # Compute the predictions (class indices with highest probabilities).
            _, preds = torch.max(outputs, dim=1)

            # Compute the softmax probabilities of predictions.
            probs = F.softmax(outputs, dim=1)

            # Extend the lists with the values from the current batch.
            review_texts.extend(texts)
            predictions.extend(preds)
            prediction_probs.extend(probs)
            real_values.extend(targets)

    # Stack the lists of tensors and move them to the CPU.
    predictions = torch.stack(predictions).cpu()
    prediction_probs = torch.stack(prediction_probs).cpu()
    real_values = torch.stack(real_values).cpu()

    return review_texts, predictions, prediction_probs, real_values

y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(
  model,
  test_data_loader, device
)

print(classification_report(y_test, y_pred, target_names=class_names))

def show_confusion_matrix(confusion_matrix, class_names):
    # Create a heatmap using seaborn.
    hmap = sns.heatmap(confusion_matrix, annot=True, fmt="d", cmap="Blues")

    # Rotate and align the tick labels for better visibility.
    hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')
    hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')

    # Set the ylabel and xlabel for the confusion matrix.
    plt.ylabel('True sentiment')
    plt.xlabel('Predicted sentiment')


# Calculate the confusion matrix.
cm = confusion_matrix(y_test, y_pred)

# Create a DataFrame for the confusion matrix with class names as index and columns.
df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)

# Call the function to show the confusion matrix using the DataFrame.
show_confusion_matrix(df_cm, class_names)

# Define the index of the data point you want to analyze
idx = 8

# Get the review text of the data point at the specified index
review_text = y_review_texts[idx]

# Get the true sentiment label of the data point at the specified index
true_sentiment = y_test[idx]

# Create a DataFrame to store the predicted probabilities for the data point
pred_df = pd.DataFrame({
    'class_names': class_names,      # Class names representing the sentiment categories
    'values': y_pred_probs[idx]      # Predicted probabilities for each sentiment category
})

print("\n".join(wrap(review_text)))
print()
print(f'True sentiment: {class_names[true_sentiment]}')

sns.barplot(x='values', y='class_names', data=pred_df, orient='h')
plt.ylabel('sentiment')
plt.xlabel('probability')
plt.xlim([0, 1]);

"""##Prediction based on raw text

To predict on raw text, you need to follow these steps:

Preprocess the raw text to convert it into the input format expected by the model (e.g., tokenization).

Pass the preprocessed text through the trained model to obtain the predicted sentiment probabilities or class labels.
"""

review_text = "American veterans train Ukrainian volunteers"

def sentiment(emotions):
  encoded_review = tokenizer.encode_plus(
      emotions,
      max_length=MAX_LEN,
      add_special_tokens=True,
      return_token_type_ids=False,
      pad_to_max_length=True,
      return_attention_mask=True,
      return_tensors='pt',
      )

  input_ids = encoded_review['input_ids'].to(device)
  attention_mask = encoded_review['attention_mask'].to(device)

  output = model(input_ids, attention_mask)
  _, prediction = torch.max(output, dim=1)

  return class_names[prediction]

  # print(f'Review text: {review_text}')
  # print(f'Sentiment  : {class_names[prediction]}')



sentiment(review_text)

"""### Analyse of Tweets with "putin" hashtag"""

putin_data_df = pd.read_csv("/content/drive/MyDrive/Opinion Mining of Ukraine-Russia War Tweets Using Deep Learning/Verlon # data/putin_text.csv")

putin_data_df.head()

putin_data_df['sentiment'] = putin_data_df['text'].apply(sentiment)

putin_data_df['sentiment']

putin_data_df.head()

# Assuming the 'sentiment' column of DataFrame df contains sentiment labels (0, 1, 2)
ax = sns.countplot(data= putin_data_df, x='sentiment')

# Set the x-axis label
plt.xlabel('Tweets with "putin" hashtag')

# Class_names is a list containing the corresponding class labels
class_names = ['negative', 'neutral', 'positive']

# Set the x-axis tick labels to the class names
ax.set_xticklabels(class_names)

# Show the plot
plt.show()

putin_data_df['sentiment'].value_counts()

putin_data_positive = len(putin_data_df[putin_data_df["sentiment"] == 'positive'].value_counts())
putin_data_neutral = len(putin_data_df[putin_data_df["sentiment"] == 'neutral'].value_counts())
putin_data_negative = len(putin_data_df[putin_data_df["sentiment"] == 'negative'].value_counts())

print(putin_data_positive)
print(putin_data_neutral)
print(putin_data_negative)

percent_putin_data_positive = putin_data_positive / 116
percent_putin_data_neutral = putin_data_neutral / 116
percent_putin_data_negative = putin_data_negative / 116

print(percent_putin_data_positive)
print(percent_putin_data_neutral)
print(percent_putin_data_negative)

labels = 'Positive', 'Neutral', 'Negative'
sizes = percent_putin_data_positive, percent_putin_data_neutral, percent_putin_data_negative
fig1, ax1 = plt.subplots()
ax1.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)
ax1.axis('equal')
plt.xlabel('Tweets with "putin" hashtag')
plt.show()

"""##Analyse of Tweets with "Zelensky" hashtag"""

zelensky_data_df = pd.read_csv("/content/drive/MyDrive/Opinion Mining of Ukraine-Russia War Tweets Using Deep Learning/Verlon # data/Zelensky_text.csv")

zelensky_data_df.head()

zelensky_data_df['sentiment'] = zelensky_data_df['text'].apply(sentiment)

zelensky_data_df['sentiment']

zelensky_data_df.head()

# Assuming the 'sentiment' column of DataFrame df contains sentiment labels (0, 1, 2)
ax = sns.countplot(data=zelensky_data_df, x='sentiment')

# Set the x-axis label
plt.xlabel('Tweets with "zelensky" hashtag')

# Class_names is a list containing the corresponding class labels
class_names = ['negative', 'neutral', 'positive']

# Set the x-axis tick labels to the class names
ax.set_xticklabels(class_names)

# Show the plot
plt.show()

zelensky_data_df['sentiment'].value_counts()

zelensky_data_positive = len(zelensky_data_df[zelensky_data_df["sentiment"] == 'positive'].value_counts())
zelensky_data_neutral = len(zelensky_data_df[zelensky_data_df["sentiment"] == 'neutral'].value_counts())
zelensky_data_negative = len(zelensky_data_df[zelensky_data_df["sentiment"] == 'negative'].value_counts())

print(zelensky_data_positive)
print(zelensky_data_neutral)
print(zelensky_data_negative)

percent_zelensky_data_positive = zelensky_data_positive / 18
percent_zelensky_data_neutral = zelensky_data_neutral / 18
percent_zelensky_data_negative = zelensky_data_negative / 18

print(percent_zelensky_data_positive)
print(percent_zelensky_data_neutral)
print(percent_zelensky_data_negative)

labels = 'Positive', 'Neutral', 'Negative'
sizes = percent_zelensky_data_positive, percent_zelensky_data_neutral, percent_zelensky_data_negative
fig1, ax1 = plt.subplots()
ax1.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)
ax1.axis('equal')
plt.xlabel('Tweets with "Zelensky" hashtag')
plt.show()

"""##Analyse of Tweets with "Ukraine" hashtag"""

Ukraine_data_df = pd.read_csv("/content/drive/MyDrive/Opinion Mining of Ukraine-Russia War Tweets Using Deep Learning/Verlon # data/ukraine_text.csv")

Ukraine_data_df.head()

Ukraine_data_df['sentiment'] = Ukraine_data_df['text'].apply(sentiment)

Ukraine_data_df['sentiment']

Ukraine_data_df.head()

# Assuming the 'sentiment' column of DataFrame df contains sentiment labels (0, 1, 2)
ax = sns.countplot(data=Ukraine_data_df, x='sentiment')

# Set the x-axis label
plt.xlabel('Tweets with "Ukraine" hashtag')

# Class_names is a list containing the corresponding class labels
class_names = ['negative', 'neutral', 'positive']

# Set the x-axis tick labels to the class names
ax.set_xticklabels(class_names)

# Show the plot
plt.show()

Ukraine_data_df['sentiment'].value_counts()

Ukraine_data_positive = len(Ukraine_data_df[Ukraine_data_df["sentiment"] == 'positive'].value_counts())
Ukraine_data_neutral = len(Ukraine_data_df[Ukraine_data_df["sentiment"] == 'neutral'].value_counts())
Ukraine_data_negative = len(Ukraine_data_df[Ukraine_data_df["sentiment"] == 'negative'].value_counts())

print(Ukraine_data_positive)
print(Ukraine_data_neutral)
print(Ukraine_data_negative)

percent_Ukraine_data_positive = Ukraine_data_positive / 36
percent_Ukraine_data_neutral = Ukraine_data_neutral / 36
percent_Ukraine_data_negative = Ukraine_data_negative / 36

print(percent_Ukraine_data_positive)
print(percent_Ukraine_data_neutral)
print(percent_Ukraine_data_negative)

labels = 'Positive', 'Neutral', 'Negative'
sizes = percent_Ukraine_data_positive, percent_Ukraine_data_neutral, percent_Ukraine_data_negative
fig1, ax1 = plt.subplots()
ax1.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)
ax1.axis('equal')
plt.xlabel('Tweets with "Ukraine" hashtag')
plt.show()

"""## Analyse of Tweets with "Russia" hashtag"""

Russia_data_df = pd.read_csv("/content/drive/MyDrive/Opinion Mining of Ukraine-Russia War Tweets Using Deep Learning/Verlon # data/Russia_text.csv")

Russia_data_df.head()

Russia_data_df['sentiment'] = Russia_data_df['text'].apply(sentiment)

Russia_data_df['sentiment']

Russia_data_df.head()

# Assuming the 'sentiment' column of DataFrame df contains sentiment labels (0, 1, 2)
ax = sns.countplot(data=Russia_text_df, x='sentiment')

# Set the x-axis label
plt.xlabel('Tweets with "putin" hashtag')

# Class_names is a list containing the corresponding class labels
class_names = ['negative', 'neutral', 'positive']

# Set the x-axis tick labels to the class names
ax.set_xticklabels(class_names)

# Show the plot
plt.show()

Russia_data_df['sentiment'].value_counts()

Russia_data_positive = len(Russia_data_df[Russia_data_df["sentiment"] == 'positive'].value_counts())
Russia_data_neutral = len(Russia_data_df[Russia_data_df["sentiment"] == 'neutral'].value_counts())
Russia_data_negative = len(Russia_data_df[Russia_data_df["sentiment"] == 'negative'].value_counts())

print(Russia_data_positive)
print(Russia_data_neutral)
print(Russia_data_negative)

percent_Russia_data_positive = Russia_data_positive / 250
percent_Russia_data_neutral = Russia_data_neutral / 250
percent_Russia_data_negative = Russia_data_negative / 250

print(percent_Russia_data_positive)
print(percent_Russia_data_neutral)
print(percent_Russia_data_negative)

labels = 'Positive', 'Neutral', 'Negative'
sizes = percent_Russia_data_positive, percent_Russia_data_neutral, percent_Russia_data_negative
fig1, ax1 = plt.subplots()
ax1.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)
ax1.axis('equal')
plt.xlabel('Tweets with "Russia" hashtag')
plt.show()

