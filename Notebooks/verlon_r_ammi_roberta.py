# -*- coding: utf-8 -*-
"""Verlon_R_AMMI_RoBERTa.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Y3nnTFNRuul8aQnskYfdjQfgUbfgPOVb

# Tite : Opinion Mining of Ukraine-Russia War Tweets Using Deep Learning

## Let us perform Sentiment Analysis with RoBERTa using using Huggingface and PyTorch.

## Setup
"""

!pip install -q -U watermark

!pip install -qq transformers

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext watermark
# %watermark -v -p numpy,pandas,torch,transformers

"""##Import Librairies & Configuration"""

# Commented out IPython magic to ensure Python compatibility.

import transformers
#from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup

from transformers import RobertaModel, RobertaTokenizer, AdamW, get_linear_schedule_with_warmup
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
import torch

import numpy as np
import pandas as pd
import seaborn as sns
from pylab import rcParams
import matplotlib.pyplot as plt
from matplotlib import rc
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from collections import defaultdict
from textwrap import wrap

from torch import nn, optim
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F

# %matplotlib inline
# %config InlineBackend.figure_format='retina'

sns.set(style='whitegrid', palette='muted', font_scale=1.2)

HAPPY_COLORS_PALETTE = ["#01BEFE", "#FFDD00", "#FF7D00", "#FF006D", "#ADFF02", "#8F00FF"]

sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))

rcParams['figure.figsize'] = 8, 6

RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
torch.manual_seed(RANDOM_SEED)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
device

"""##Data Exploration"""

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv("/content/drive/MyDrive/Opinion Mining of Ukraine-Russia War Tweets Using Deep Learning/data/processed/labeled_sample_600.csv")
df.head()

df.shape

df.info()

df.isnull().sum()

df.columns

df["value"].value_counts()

df["value"]

# The 'value' column of DataFrame df contains values 0, 1, and 2
sns.countplot(data=df, x='value', order=[0, 1, 2])
plt.xlabel('review score')
plt.show()

## Define a function to convert ratings to sentiment labels
def to_sentiment(rating):
  rating = int(rating)      # Convert the rating to an integer (in case it's not already)
  if rating == 0:          # Check if the rating is 0
    return 0               # If it is, return sentiment label 0
  elif rating == 1:        # Check if the rating is 1
    return 1               # If it is, return sentiment label 1
  else:                    # If the rating is neither 0 nor 1
    return 2               # Return sentiment label 2

# Apply the to_sentiment function to the 'value' column of the DataFrame 'df'
# and create a new column named 'sentiment' to store the sentiment labels.

df['sentiment'] = df.value.apply(to_sentiment)

df['sentiment']

# Assuming the 'sentiment' column of DataFrame df contains sentiment labels (0, 1, 2)
ax = sns.countplot(data=df, x='sentiment')

# Set the x-axis label
plt.xlabel('review sentiment')

# Class_names is a list containing the corresponding class labels
class_names = ['negative', 'neutral', 'positive']

# Set the x-axis tick labels to the class names
ax.set_xticklabels(class_names)

# Show the plot
plt.show()

# Create a DataFrame containing the counts of each unique value in the 'sentiment' column using the groupby function.
grouped_counts = df.groupby(['sentiment'])['sentiment'].count()

# Convert the grouped_counts Series back to a DataFrame and rename the 'sentiment' column to 'Counts'.
# The DataFrame will have two columns: 'sentiment' (now renamed as 'Counts') and the count of each sentiment.
counts_df = pd.DataFrame(grouped_counts).rename(columns={"sentiment": "Counts"})

# Calculate the percentage of each sentiment class relative to the total number of sentiments in the DataFrame.
# The calculation is performed using the 'assign' method and a lambda function 'Percentage'.
# The 'Percentage' column is added to the counts_df DataFrame.
counts_df = counts_df.assign(Percentage=lambda x: (x.Counts / x.Counts.sum()) * 100)
counts_df

# Define the counts for each sentiment category (positive, neutral, and negative).
positive = 280
neutral = 155
negative = 165

# Creating a Pie Chart to visualize the distribution of sentiments.
# Define the labels for the pie chart, including the counts for each sentiment category.
labels = ['Positive [' + str(positive) + ']', 'Neutral [' + str(neutral) + ']', 'Negative [' + str(negative) + ']']

# Define the sizes of the pie chart slices, representing the counts for each sentiment category.
sizes = [positive, neutral, negative]

# Define colors for each sentiment category in the pie chart.
colors = ["#81F495", "#A9E4EF", "#FF3C38"]

# Create the pie chart using the sizes and colors defined above.
patches, texts = plt.pie(sizes, colors=colors, startangle=90)

# Use the default style for the plot.
plt.style.use('default')

# Add a legend to the pie chart with the specified labels.
plt.legend(labels)

# Set the title for the pie chart.
plt.title('#Number of Tweets (Positive, Negative, Neutral)')

# Ensure the pie chart is circular (aspect ratio equal) and show the plot.
plt.axis('equal')
plt.show()



# Calculate the length of each text in the 'text' column and add it as a new column 'text_len' in the DataFrame (df).
# The 'astype(str)' ensures that each text is converted to a string type before applying the 'len' function.
# The result is the number of characters in each text, which is stored in the new 'text_len' column.
df['text_len'] = df['text'].astype(str).apply(len)

# Calculate the word count of each text in the 'text' column and add it as a new column 'text_word_count' in the DataFrame (df).
# The 'lambda x: len(str(x).split())' function splits each text into words using whitespace as the delimiter,
# and then calculates the number of words in each text.
# The result is the word count for each text, which is stored in the new 'text_word_count' column.
df['text_word_count'] = df['text'].apply(lambda x: len(str(x).split()))

# Print the average length of text across all rows in the DataFrame (df).
# The 'np.mean(df['text_len'])' calculates the mean of the 'text_len' column.
# The 'round' function is used to round the average length to the nearest integer.
print("Average length of text:", round(np.mean(df['text_len'])))

# Print the average word count of text across all rows in the DataFrame (df).
# The 'np.mean(df['text_word_count'])' calculates the mean of the 'text_word_count' column.
# The 'round' function is used to round the average word count to the nearest integer.
print("Average word counts of text:", round(np.mean(df['text_word_count'])))

print(df.columns)

"""##Data Preparation"""

df.nlargest(n=50, columns=['sentiment'])["text"]

sample_txt = 'Hello, How are you? I am under the water, Please help me! sfosd'
encoding = tokenizer.encode_plus(
    sample_txt,
    max_length=32,
    add_special_tokens=True,
    pad_to_max_length=True,
    return_attention_mask=True,
    return_token_type_ids=False,
    return_tensors='pt'
)

print('Special Tokens')
print(tokenizer.sep_token, tokenizer.sep_token_id)
print(tokenizer.cls_token, tokenizer.cls_token_id)
print(tokenizer.pad_token, tokenizer.pad_token_id)
print(tokenizer.unk_token, tokenizer.unk_token_id)

print(encoding['input_ids'][0])
print(encoding['attention_mask'][0])

MAX_LEN = 160

from torch.utils.data import Dataset, DataLoader
from torch import nn
import torch.nn.functional as F

class TwitterDataset(Dataset):

  def __init__(self, text, value, tokenizer, max_len):
    self.text = text
    self.value = value
    self.tokenizer = tokenizer
    self.max_len = max_len

  def __len__(self):
    return len(self.text)

  def __getitem__(self, index):
    text = str(self.text[index])
    value = self.value[index]

    encoding = tokenizer.encode_plus(
        text,
        max_length=self.max_len,
        add_special_tokens=True,
        pad_to_max_length=True,
        return_attention_mask=True,
        return_token_type_ids=False,
        return_tensors='pt'
    )

    return {
        'text': text,
        'input_ids': encoding['input_ids'].flatten(),
        'attention_mask': encoding['attention_mask'].flatten(),
        'score': torch.tensor(value, dtype=torch.long)
    }

MAX_LEN=160
BATCH_SIZE = 16 # BERT paper suggests a batch size of 16 or 32
EPOCHS = 10
class_names=3

df_train, df_test = train_test_split(df, test_size=0.1, random_state=RANDOM_SEED)
df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=RANDOM_SEED)

df_train.shape, df_val.shape, df_test.shape

from torch.utils.data import DataLoader

def create_dataloader(df, tokenizer, max_len, batch_size):
  ds = TwitterDataset(text=df.text.to_numpy(),
                      value=df.value.to_numpy(),
                      tokenizer=tokenizer,
                      max_len=max_len)
  return DataLoader(ds,
                    batch_size=batch_size,
                    num_workers=2)

train_data_loader = create_dataloader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)
val_data_loader = create_dataloader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)
test_data_loader = create_dataloader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)

# Retrieve the next batch of data from the 'train_data_loader' using the 'next' function and store it in the 'data' variable.
data = next(iter(train_data_loader))

#  data.keys()  accesses the keys of the 'data' dictionary, showing the available items in the batch.
# The 'data' dictionary typically contains items like 'input_ids', 'attention_mask', and 'targets' used in model training.
# These items represent tokenized input, attention masks, and target labels, respectively.
# You can print 'data.keys()' to see the available keys and understand the structure of the batch.
data.keys()

batch_example = next(iter(train_data_loader))

print(batch_example['input_ids'][0])
print(batch_example['text'][13])
print(batch_example['attention_mask'][0])
print(batch_example['score'][0])

batch_example['input_ids'].shape

PRE_TRAINED_MODEL_NAME = 'roberta-base'

tokenizer = RobertaTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)

roberta_model = RobertaModel.from_pretrained(PRE_TRAINED_MODEL_NAME)

# [batch_size, max_seq_length, 768], 768 dimension embedding for each token in the given sentence
# Pooler output will give output of dimension [1, 1, 768]--> [1, 768], which is the embedding of [CLS] token.
# In general people use 'pooled output' of the sentence and use it for text classification

last_hidden_state, pooled_output = roberta_model(
    input_ids=encoding['input_ids'],
    attention_mask=encoding['attention_mask'],
    return_dict=False
)
print(last_hidden_state.shape)
print(pooled_output.shape)

roberta_model.config.hidden_size

class SentimentClassifier(nn.Module):
    def __init__(self, n_classes):
        super(SentimentClassifier, self).__init__()
        self.roberta = RobertaModel.from_pretrained(PRE_TRAINED_MODEL_NAME)
        self.drop = nn.Dropout(p=0.3)
        self.out = nn.Linear(self.roberta.config.hidden_size, n_classes)

    def forward(self, input_ids, attention_mask):
        roberta_output = self.roberta(
            input_ids=input_ids,
            attention_mask=attention_mask,
            return_dict=False
        )
        pooled_output = roberta_output[1]  # Grab the pooled output from the tuple
        output = self.drop(pooled_output)
        output = self.out(output)

        return output

model = SentimentClassifier(class_names)
model = model.to(device)

input_ids = data['input_ids'].to(device)
attention_mask = data['attention_mask'].to(device)

print(input_ids.shape) # batch size x seq length
print(attention_mask.shape) # batch size x seq length

batch_example['score']

# Forward pass through the model
outputs = model(input_ids, attention_mask)

# Apply softmax to the outputs to get probabilities for each class
outputs_probs = F.softmax(outputs, dim=1)

# Find the predicted class for each input by selecting the index with the highest probability
_, preds = torch.max(outputs_probs, dim=1)

# Print the first 11 output tensors
print("Output tensors:", outputs[:11], '\n')

# Print the first 11 sets of class probabilities
print("Class probabilities:", outputs_probs[:11], '\n')

# Print the predicted classes
print("Predicted classes:", preds)

"""## Model Training"""

Epochs = 10

# Define the optimizer for the model using the AdamW optimizer with a learning rate of 2e-5 and no bias correction.
optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)

# Calculate the total number of training steps across all epochs.
total_steps = len(train_data_loader) * Epochs

# Create a learning rate scheduler using the linear scheduler with warm-up.
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=0,
    num_training_steps=total_steps
)

# Define the loss function for the classification task, which is the cross-entropy loss.
# The loss function is moved to the device (e.g., GPU) for computation.
loss_fn = nn.CrossEntropyLoss().to(device)

from tqdm.auto import tqdm

def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):

  model = model.train()

  losses = []
  correct_predictions = 0

  for batch in tqdm(data_loader):
    input_ids = batch['input_ids'].to(device)
    attention_mask = batch['attention_mask'].to(device)
    score = batch['score'].to(device)

    outputs = model(
        input_ids,
        attention_mask=attention_mask
    )

    _, preds = torch.max(F.softmax(outputs), dim=1)
    loss = loss_fn(outputs, score)

    correct_predictions += torch.sum(preds == score)
    losses.append(loss.item())

    loss.backward() # Compute dloss/dx for every parameter x
    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # A way to combat exploding gradients (gradients becoming too large), similar to BERT paper
    optimizer.step() # Update parameters
    scheduler.step() # Reduce learning rate

    """
     Zero out gradients so that you do the parameter update correctly.
     Else the gradient would point in some other direction than towards minumum
     We do this because by default loss.backward() accumulates gradients
    """
    optimizer.zero_grad()

  accuracy = correct_predictions.double() / n_examples
  loss = np.mean(losses)

  return accuracy, loss

def eval_model(model, data_loader, loss_fn, device, n_examples):

  model = model.eval()

  losses = []
  correct_predictions = 0

  with torch.no_grad(): # Disable gradient functions so torch is faster
    for batch in data_loader:
      input_ids = batch['input_ids'].to(device)
      attention_mask = batch['attention_mask'].to(device)
      score = batch['score'].to(device)

      outputs = model(
          input_ids,
          attention_mask=attention_mask
      )

      _, preds = torch.max(F.softmax(outputs), dim=1)
      loss = loss_fn(outputs, score)

      correct_predictions += torch.sum(preds == score)
      losses.append(loss.item())

    accuracy = correct_predictions.double() / n_examples
    loss = np.mean(losses)

  return accuracy, loss

print(len(df_train))
print(len(df_val))

#%%time
from collections import defaultdict

history = defaultdict(list)
best_accuracy = 0

for epoch in range(Epochs):
  print(f'Epoch {epoch + 1}/{Epochs}')
  print('-' * 10)

  train_acc, train_loss = train_epoch(model, train_data_loader, loss_fn, optimizer, device, scheduler, len(df_train) )

  print(f'Train loss {train_loss} Train accuracy {train_acc}')

  val_acc, val_loss = eval_model(
      model=model,
      data_loader=val_data_loader,
      loss_fn=loss_fn,
      device=device,
      n_examples=len(df_val)
  )

  print(f'Val   loss {val_loss} Val accuracy {val_acc}')
  print()

  history['train_acc'].append(train_acc)
  history['train_loss'].append(train_loss)

  history['val_acc'].append(val_acc)
  history['val_loss'].append(val_loss)

  if val_acc > best_accuracy:
    torch.save(model.state_dict(), 'best_model1_state.bin')
    best_accuracy = val_acc

"""## Let us make evaluation"""

model = SentimentClassifier(3)
model.load_state_dict(torch.load('best_model1_state.bin'))
model = model.to(device)

test_acc, test_loss =  eval_model(model, test_data_loader, loss_fn, device, len(df_test))
test_acc

# Evaluate the trained model on the test dataset and obtain accuracy.
test_acc, _ = eval_model(
    model,
    test_data_loader,
    loss_fn,
    device,
    len(df_test)
)

# Convert the test accuracy to a Python float value and print it.
test_accuracy_value = test_acc.item()
print(f"Test accuracy: {test_accuracy_value:.4f}")

"""The accuracy is about 16% lower on the test set. Our model does not seems to generalize well.

We'll define a helper function to get the predictions from our model:
"""

# Move the tensors to CPU to avoid converting CUDA tensors to NumPy arrays
train_acc = torch.tensor(history['train_acc']).cpu()
val_acc = torch.tensor(history['val_acc']).cpu()

# Plot the training and validation accuracy
plt.plot(train_acc, label='train accuracy')
plt.plot(val_acc, label='validation accuracy')

plt.title('Training history')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend()
plt.ylim([0, 1])

plt.show()

"""The training accuracy starts to approach 100% after 8 epochs or so. You might try to fine-tune the parameters a bit more."""

# Move the tensors to CPU to avoid converting CUDA tensors to NumPy arrays
train_loss = torch.tensor(history['train_loss']).cpu()
val_loss = torch.tensor(history['val_loss']).cpu()

# Plot the training and validation accuracy
plt.plot(train_loss, label='train loss')
plt.plot(val_loss, label='validation loss')

plt.title('Training history')
plt.ylabel('loss')
plt.xlabel('Epoch')
plt.legend()
plt.ylim([0, 1])

plt.show()

import torch
import torch.nn.functional as F


def get_predictions(model, data_loader):
    model = model.eval()

    review_texts = []
    predictions = []
    prediction_probs = []
    real_values = []

    with torch.no_grad():
        for batch in data_loader:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            targets = batch["score"].to(device)

            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask
            )
            _, preds = torch.max(outputs, dim=1)

            probs = F.softmax(outputs, dim=1)

            predictions.extend(preds.tolist())
            prediction_probs.extend(probs.tolist())
            real_values.extend(targets.tolist())

    return predictions, prediction_probs, real_values

y_pred, y_pred_probs, y_test = get_predictions(
    model,
    test_data_loader
)

# Define the class names as a list of strings
class_names = ['negative  ', 'neutral', 'positive ']

# Calculate the classification report.
report = classification_report(y_test, y_pred, target_names=class_names)

# Print the classification report.
print(report)

def show_confusion_matrix(confusion_matrix):
  hmap = sns.heatmap(confusion_matrix, annot=True, fmt="d", cmap="Blues")
  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')
  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')
  plt.ylabel('True sentiment')
  plt.xlabel('Predicted sentiment');

cm = confusion_matrix(y_test, y_pred)
df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)
show_confusion_matrix(df_cm)